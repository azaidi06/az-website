{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"end2end \u2014 Golf Swing Detection","text":"<p>Automatically find the top-of-backswing and ball-contact frames in golf videos using wrist keypoints from mmpose and Savitzky-Golay signal processing.</p> <p>No ML model, no GPU, no training data \u2014 just wrist positions and a pair of smoothing filters.</p>"},{"location":"#pipeline","title":"Pipeline","text":"<pre><code>Raw wrist keypoints (x, y per frame)\n  \u2192 Interpolate low-confidence frames\n  \u2192 Combined signal: (x_L + x_R)/2 + (y_L + y_R)/2\n  \u2192 Fine smooth (9 frames / 150ms) + Coarse smooth (61 frames / 1.0s)\n  \u2192 find_peaks on inverted signal (anchors)\n  \u2192 Backward search on coarse signal (candidate regions)\n  \u2192 Backswing scoring (smooth approach + sharp departure)\n  \u2192 Apex refinement (+15 frames on fine signal)\n  \u2192 Five-stage filtering (dedup, end-trim, merge, MAD, follow-through)\n  \u2192 Contact search (forward 10-90 frames for max)\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from end2end import detect_backswings, detect_contacts\n\n# Detect backswing tops\nresult = detect_backswings(\"keypoints/IMG_1171.pkl\", \"IMG_1171.mp4\")\nprint(f\"{result.n_swings} swings at frames {result.peak_frames}\")\n\n# Find contact points\ncontacts = detect_contacts(result)\nprint(f\"{contacts.n_contacts} contacts at frames {contacts.contact_frames}\")\n</code></pre>"},{"location":"#cli","title":"CLI","text":"<pre><code># Backswing only\npython -m end2end.run_batch ../saugusta --no-clips --no-pushover\n\n# Backswing + contact + CSV export\npython -m end2end.run_batch ../oct25 --contact --csv --no-clips --no-pushover\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>Configuration \u2014 <code>Config</code>, <code>DetectionResult</code>, <code>ContactResult</code></li> <li>I/O \u2014 loading keypoint pickles and video metadata</li> <li>Signal Processing \u2014 interpolation and combined arc signal</li> <li>Peak Detection \u2014 anchor detection, scoring, refinement</li> <li>Filters \u2014 five-stage post-processing pipeline</li> <li>Contact Detection \u2014 forward search for ball impact</li> <li>Pipeline \u2014 thin orchestrators (<code>detect_backswings</code>, <code>detect_contacts</code>)</li> <li>Visualization \u2014 skeleton grids, signal plots, clips</li> <li>Batch Processing \u2014 CLI runner and CSV export</li> </ul>"},{"location":"reference/config/","title":"Configuration","text":"<p>Pipeline parameters and result containers.</p>"},{"location":"reference/config/#end2end.config","title":"<code>config</code>","text":"<p>Configuration and result dataclasses for the detection pipeline.</p> <p>Provides a single frozen <code>Config</code> for all tunable parameters and two mutable result containers (<code>DetectionResult</code>, <code>ContactResult</code>) returned by the detection functions.</p>"},{"location":"reference/config/#end2end.config.Config","title":"<code>Config</code>  <code>dataclass</code>","text":"<p>Frozen configuration for the backswing / contact detection pipeline.</p> <p>All defaults are tuned on two datasets (7 + 10 videos, 92 total swings at 60 fps).  Frame-count parameters assume 60 fps \u2014 scale proportionally for other frame rates.</p> <p>Attributes:</p> Name Type Description <code>savgol_window</code> <code>int</code> <p>Fine Savitzky-Golay window in frames (~150 ms at 60 fps). Removes frame-to-frame jitter while preserving swing shape.</p> <code>savgol_poly</code> <code>int</code> <p>Polynomial order for the fine Savitzky-Golay filter.</p> <code>coarse_window</code> <code>int</code> <p>Coarse Savitzky-Golay window in frames (~1.0 s at 60 fps).  Wide enough to flatten within-swing oscillations so <code>find_peaks</code> can locate swing regions.</p> <code>coarse_poly</code> <code>int</code> <p>Polynomial order for the coarse Savitzky-Golay filter.</p> <code>peak_prominence</code> <code>int</code> <p>Minimum prominence (pixels) for <code>find_peaks</code> on the inverted signal.  A swing must produce at least this much dip.</p> <code>peak_distance</code> <code>int</code> <p>Minimum distance (frames, ~8.3 s) between anchor peaks.</p> <code>look_behind</code> <code>int</code> <p>Frames behind an anchor examined when scoring backswing candidates (approach jitter).</p> <code>look_ahead</code> <code>int</code> <p>Frames ahead of an anchor examined when scoring backswing candidates (departure drop).</p> <code>search_back</code> <code>int</code> <p>Maximum frames (~10 s) to search backward from an anchor on the coarse signal for the true backswing candidate.</p> <code>refine_window</code> <code>int</code> <p>Forward search window (frames, ~250 ms) on the fine signal to correct the coarse-smoothing bias.</p> <code>min_swing_gap</code> <code>int</code> <p>Merge threshold (frames, ~10 s) \u2014 peaks closer than this are merged, keeping the deeper one.</p> <code>end_of_video_pct</code> <code>float</code> <p>Fraction of video at the end to suppress (camera noise / golfer walking away).</p> <code>xy_outlier_mad_thresh</code> <code>float</code> <p>MAD multiplier for the x+y outlier filter.</p> <code>xy_outlier_min_peaks</code> <code>int</code> <p>Minimum peak count before MAD filtering is applied.</p> <code>xy_outlier_mad_floor</code> <code>int</code> <p>MAD floor (pixels) to prevent an unreasonably tight threshold when swings are very consistent.</p> <code>xoff_mad_thresh</code> <code>float</code> <p>MAD multiplier for the follow-through rejection filter (wrist-to-shoulder horizontal offset).</p> <code>xoff_mad_floor</code> <code>int</code> <p>MAD floor (pixels) for the follow-through filter.</p> <code>left_wrist</code> <code>int</code> <p>COCO-17 keypoint index for the left wrist.</p> <code>right_wrist</code> <code>int</code> <p>COCO-17 keypoint index for the right wrist.</p> <code>left_shoulder</code> <code>int</code> <p>COCO-17 keypoint index for the left shoulder.</p> <code>right_shoulder</code> <code>int</code> <p>COCO-17 keypoint index for the right shoulder.</p> <code>conf_threshold</code> <code>float</code> <p>Minimum keypoint confidence; frames below this are linearly interpolated.</p> <code>contact_search_min</code> <code>int</code> <p>Minimum frames (~170 ms) after a backswing to begin searching for contact.</p> <code>contact_search_max</code> <code>int</code> <p>Maximum frames (~1.5 s) after a backswing to search for contact.</p> <code>contact_savgol_window</code> <code>int</code> <p>Savitzky-Golay window (frames, ~83 ms) for contact-point smoothing.</p> <code>contact_savgol_poly</code> <code>int</code> <p>Polynomial order for the contact smoother.</p> <code>min_expected_swings</code> <code>int</code> <p>Flag a video if fewer swings than this are found.</p> <code>max_expected_swings</code> <code>int</code> <p>Flag a video if more swings than this are found.</p> <code>low_conf_window</code> <code>int</code> <p>Half-window (frames) around a peak for confidence checking.</p> <code>low_conf_threshold</code> <code>float</code> <p>Mean wrist confidence below this triggers a flag.</p> <code>close_gap_seconds</code> <code>float</code> <p>Flag if two successive swings are closer than this (seconds).</p> <code>flag_mad_threshold</code> <code>float</code> <p>MAD z-score above which a peak's x+y value is flagged as suspicious.</p> <code>downswing_outlier_frames</code> <code>int</code> <p>Frame threshold used when exporting CSV to flag unusually long downswings.</p> Source code in <code>end2end/config.py</code> <pre><code>@dataclass(frozen=True)\nclass Config:\n    \"\"\"Frozen configuration for the backswing / contact detection pipeline.\n\n    All defaults are tuned on two datasets (7 + 10 videos, 92 total swings at\n    60 fps).  Frame-count parameters assume 60 fps \u2014 scale proportionally for\n    other frame rates.\n\n    Attributes:\n        savgol_window: Fine Savitzky-Golay window in frames (~150 ms at 60 fps).\n            Removes frame-to-frame jitter while preserving swing shape.\n        savgol_poly: Polynomial order for the fine Savitzky-Golay filter.\n        coarse_window: Coarse Savitzky-Golay window in frames (~1.0 s at\n            60 fps).  Wide enough to flatten within-swing oscillations so\n            ``find_peaks`` can locate swing *regions*.\n        coarse_poly: Polynomial order for the coarse Savitzky-Golay filter.\n        peak_prominence: Minimum prominence (pixels) for ``find_peaks`` on the\n            inverted signal.  A swing must produce at least this much dip.\n        peak_distance: Minimum distance (frames, ~8.3 s) between anchor peaks.\n        look_behind: Frames behind an anchor examined when scoring backswing\n            candidates (approach jitter).\n        look_ahead: Frames ahead of an anchor examined when scoring backswing\n            candidates (departure drop).\n        search_back: Maximum frames (~10 s) to search backward from an anchor\n            on the coarse signal for the true backswing candidate.\n        refine_window: Forward search window (frames, ~250 ms) on the fine\n            signal to correct the coarse-smoothing bias.\n        min_swing_gap: Merge threshold (frames, ~10 s) \u2014 peaks closer than\n            this are merged, keeping the deeper one.\n        end_of_video_pct: Fraction of video at the end to suppress (camera\n            noise / golfer walking away).\n        xy_outlier_mad_thresh: MAD multiplier for the x+y outlier filter.\n        xy_outlier_min_peaks: Minimum peak count before MAD filtering is\n            applied.\n        xy_outlier_mad_floor: MAD floor (pixels) to prevent an unreasonably\n            tight threshold when swings are very consistent.\n        xoff_mad_thresh: MAD multiplier for the follow-through rejection\n            filter (wrist-to-shoulder horizontal offset).\n        xoff_mad_floor: MAD floor (pixels) for the follow-through filter.\n        left_wrist: COCO-17 keypoint index for the left wrist.\n        right_wrist: COCO-17 keypoint index for the right wrist.\n        left_shoulder: COCO-17 keypoint index for the left shoulder.\n        right_shoulder: COCO-17 keypoint index for the right shoulder.\n        conf_threshold: Minimum keypoint confidence; frames below this are\n            linearly interpolated.\n        contact_search_min: Minimum frames (~170 ms) after a backswing to\n            begin searching for contact.\n        contact_search_max: Maximum frames (~1.5 s) after a backswing to\n            search for contact.\n        contact_savgol_window: Savitzky-Golay window (frames, ~83 ms) for\n            contact-point smoothing.\n        contact_savgol_poly: Polynomial order for the contact smoother.\n        min_expected_swings: Flag a video if fewer swings than this are found.\n        max_expected_swings: Flag a video if more swings than this are found.\n        low_conf_window: Half-window (frames) around a peak for confidence\n            checking.\n        low_conf_threshold: Mean wrist confidence below this triggers a flag.\n        close_gap_seconds: Flag if two successive swings are closer than this\n            (seconds).\n        flag_mad_threshold: MAD z-score above which a peak's x+y value is\n            flagged as suspicious.\n        downswing_outlier_frames: Frame threshold used when exporting CSV to\n            flag unusually long downswings.\n    \"\"\"\n\n    # Smoothing\n    savgol_window: int = 9\n    savgol_poly: int = 3\n    coarse_window: int = 61\n    coarse_poly: int = 3\n    # Peak detection\n    peak_prominence: int = 300\n    peak_distance: int = 500\n    look_behind: int = 60\n    look_ahead: int = 30\n    search_back: int = 600\n    refine_window: int = 15\n    # Post-processing filters\n    min_swing_gap: int = 600\n    end_of_video_pct: float = 0.03\n    xy_outlier_mad_thresh: float = 3.0\n    xy_outlier_min_peaks: int = 3\n    xy_outlier_mad_floor: int = 50\n    xoff_mad_thresh: float = 3.0\n    xoff_mad_floor: int = 20\n    # Keypoint constants\n    left_wrist: int = 9\n    right_wrist: int = 10\n    left_shoulder: int = 5\n    right_shoulder: int = 6\n    conf_threshold: float = 0.3\n    # Contact detection\n    contact_search_min: int = 10\n    contact_search_max: int = 90\n    contact_savgol_window: int = 5\n    contact_savgol_poly: int = 2\n    # Problem flagging\n    min_expected_swings: int = 2\n    max_expected_swings: int = 15\n    low_conf_window: int = 5\n    low_conf_threshold: float = 0.4\n    close_gap_seconds: float = 8.0\n    flag_mad_threshold: float = 2.0\n    # CSV export\n    downswing_outlier_frames: int = 40\n</code></pre>"},{"location":"reference/config/#end2end.config.DetectionResult","title":"<code>DetectionResult</code>  <code>dataclass</code>","text":"<p>Container for backswing detection output from a single video.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Video filename stem (e.g. <code>\"IMG_1171\"</code>).</p> <code>peak_frames</code> <code>ndarray</code> <p>1-D array of frame indices for detected backswing tops.</p> <code>smoothed</code> <code>ndarray</code> <p>Fine-smoothed combined signal (same length as keypoint frames).</p> <code>combined</code> <code>ndarray</code> <p>Raw combined signal <code>(x_L + x_R)/2 + (y_L + y_R)/2</code> after low-confidence interpolation.</p> <code>fps</code> <code>float</code> <p>Video frame rate.</p> <code>total_frames</code> <code>int</code> <p>Total number of video frames.</p> <code>filter_log</code> <code>list</code> <p>Human-readable log of which filters fired and how many peaks they removed.</p> <code>pkl_data</code> <code>dict</code> <p>Full keypoint dictionary loaded from the pickle file.</p> <code>pkl_path</code> <code>str</code> <p>Path to the keypoint pickle file.</p> <code>mov_path</code> <code>str</code> <p>Path to the source video file.</p> Source code in <code>end2end/config.py</code> <pre><code>@dataclass\nclass DetectionResult:\n    \"\"\"Container for backswing detection output from a single video.\n\n    Attributes:\n        name: Video filename stem (e.g. ``\"IMG_1171\"``).\n        peak_frames: 1-D array of frame indices for detected backswing tops.\n        smoothed: Fine-smoothed combined signal (same length as keypoint\n            frames).\n        combined: Raw combined signal ``(x_L + x_R)/2 + (y_L + y_R)/2``\n            after low-confidence interpolation.\n        fps: Video frame rate.\n        total_frames: Total number of video frames.\n        filter_log: Human-readable log of which filters fired and how many\n            peaks they removed.\n        pkl_data: Full keypoint dictionary loaded from the pickle file.\n        pkl_path: Path to the keypoint pickle file.\n        mov_path: Path to the source video file.\n    \"\"\"\n\n    name: str\n    peak_frames: np.ndarray\n    smoothed: np.ndarray\n    combined: np.ndarray\n    fps: float\n    total_frames: int\n    filter_log: list = field(default_factory=list)\n    pkl_data: dict = field(default=None, repr=False)\n    pkl_path: str = \"\"\n    mov_path: str = \"\"\n\n    @property\n    def n_swings(self):\n        \"\"\"Number of detected backswing peaks.\"\"\"\n        return len(self.peak_frames)\n</code></pre>"},{"location":"reference/config/#end2end.config.DetectionResult.n_swings","title":"<code>n_swings</code>  <code>property</code>","text":"<p>Number of detected backswing peaks.</p>"},{"location":"reference/config/#end2end.config.ContactResult","title":"<code>ContactResult</code>  <code>dataclass</code>","text":"<p>Container for contact-point detection output from a single video.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Video filename stem.</p> <code>contact_frames</code> <code>ndarray</code> <p>1-D array of frame indices for detected contact points.</p> <code>backswing_result</code> <code>DetectionResult</code> <p>The upstream <code>DetectionResult</code> that seeded the contact search.</p> <code>smoothed</code> <code>ndarray</code> <p>Tightly smoothed combined signal used for contact detection.</p> <code>filter_log</code> <code>list</code> <p>Human-readable log of any deduplication that occurred.</p> Source code in <code>end2end/config.py</code> <pre><code>@dataclass\nclass ContactResult:\n    \"\"\"Container for contact-point detection output from a single video.\n\n    Attributes:\n        name: Video filename stem.\n        contact_frames: 1-D array of frame indices for detected contact\n            points.\n        backswing_result: The upstream ``DetectionResult`` that seeded the\n            contact search.\n        smoothed: Tightly smoothed combined signal used for contact detection.\n        filter_log: Human-readable log of any deduplication that occurred.\n    \"\"\"\n\n    name: str\n    contact_frames: np.ndarray\n    backswing_result: DetectionResult\n    smoothed: np.ndarray\n    filter_log: list = field(default_factory=list)\n\n    @property\n    def n_contacts(self):\n        \"\"\"Number of detected contact points.\"\"\"\n        return len(self.contact_frames)\n</code></pre>"},{"location":"reference/config/#end2end.config.ContactResult.n_contacts","title":"<code>n_contacts</code>  <code>property</code>","text":"<p>Number of detected contact points.</p>"},{"location":"reference/contact/","title":"Contact Detection","text":"<p>Forward search from backswing tops to find ball-impact frames.</p>"},{"location":"reference/contact/#end2end.contact","title":"<code>contact</code>","text":"<p>Contact-point detection \u2014 forward search from backswing tops.</p> <p>Finds the ball-impact frame by searching for the maximum of a tightly smoothed signal in a window after each backswing.</p>"},{"location":"reference/contact/#end2end.contact.detect_contact_points","title":"<code>detect_contact_points(bs_frames, combined, n_pkl, cfg)</code>","text":"<p>Find contact-point frame indices by forward search from backswings.</p> <p>For each backswing frame, searches forward <code>contact_search_min</code> to <code>contact_search_max</code> frames on a tightly smoothed signal for the maximum \u2014 the moment where the hands are fully extended (club meeting ball).</p> <p>Parameters:</p> Name Type Description Default <code>bs_frames</code> <p>1-D array of backswing-top frame indices.</p> required <code>combined</code> <p>Raw combined arc signal.</p> required <code>n_pkl</code> <p>Number of keypoint frames in the pickle.</p> required <code>cfg</code> <p><code>Config</code> instance.</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(contact_frames, smoothed)</code> where contact_frames is a</p> <p>1-D int array and smoothed is the contact-smoothed signal.</p> Source code in <code>end2end/contact.py</code> <pre><code>def detect_contact_points(bs_frames, combined, n_pkl, cfg):\n    \"\"\"Find contact-point frame indices by forward search from backswings.\n\n    For each backswing frame, searches forward ``contact_search_min`` to\n    ``contact_search_max`` frames on a tightly smoothed signal for the\n    maximum \u2014 the moment where the hands are fully extended (club meeting\n    ball).\n\n    Args:\n        bs_frames: 1-D array of backswing-top frame indices.\n        combined: Raw combined arc signal.\n        n_pkl: Number of keypoint frames in the pickle.\n        cfg: ``Config`` instance.\n\n    Returns:\n        Tuple of ``(contact_frames, smoothed)`` where *contact_frames* is a\n        1-D int array and *smoothed* is the contact-smoothed signal.\n    \"\"\"\n    smoothed = savgol_filter(combined, cfg.contact_savgol_window, cfg.contact_savgol_poly)\n    contacts = []\n    for bf in bs_frames:\n        s, e = bf + cfg.contact_search_min, min(bf + cfg.contact_search_max, n_pkl - 1)\n        if s &gt;= n_pkl:\n            continue\n        seg = smoothed[s:e + 1]\n        if len(seg) &gt; 0:\n            contacts.append(s + np.argmax(seg))\n    return np.array(contacts), smoothed\n</code></pre>"},{"location":"reference/filters/","title":"Filters","text":"<p>Composable five-stage post-processing filter pipeline.</p>"},{"location":"reference/filters/#end2end.filters","title":"<code>filters</code>","text":"<p>Composable filter pipeline \u2014 five sequential post-processing filters.</p> <p>Each filter takes peaks (and context) and returns <code>(peaks, log_entries)</code>. <code>run_all</code> chains them in order.</p>"},{"location":"reference/filters/#end2end.filters.dedup","title":"<code>dedup(peaks)</code>","text":"<p>Remove duplicate frame indices and return a sorted array.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <p>1-D array of frame indices (may contain duplicates).</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(deduped, log)</code> where deduped is a sorted 1-D int</p> <p>array and log is a list with a message if duplicates were removed.</p> Source code in <code>end2end/filters.py</code> <pre><code>def dedup(peaks):\n    \"\"\"Remove duplicate frame indices and return a sorted array.\n\n    Args:\n        peaks: 1-D array of frame indices (may contain duplicates).\n\n    Returns:\n        Tuple of ``(deduped, log)`` where *deduped* is a sorted 1-D int\n        array and *log* is a list with a message if duplicates were removed.\n    \"\"\"\n    log = []\n    if len(peaks) == 0:\n        return peaks, log\n    before = len(peaks)\n    peaks = np.array(sorted(set(peaks.tolist())))\n    if len(peaks) != before:\n        log.append(f\"Dedup: removed {before - len(peaks)} duplicate(s)\")\n    return peaks, log\n</code></pre>"},{"location":"reference/filters/#end2end.filters.trim_end","title":"<code>trim_end(peaks, total_frames, end_of_video_pct)</code>","text":"<p>Drop peaks in the last portion of the video.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <p>1-D array of peak frame indices.</p> required <code>total_frames</code> <p>Total video frame count.</p> required <code>end_of_video_pct</code> <p>Fraction of video at the end to suppress.</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(filtered_peaks, log)</code>.</p> Source code in <code>end2end/filters.py</code> <pre><code>def trim_end(peaks, total_frames, end_of_video_pct):\n    \"\"\"Drop peaks in the last portion of the video.\n\n    Args:\n        peaks: 1-D array of peak frame indices.\n        total_frames: Total video frame count.\n        end_of_video_pct: Fraction of video at the end to suppress.\n\n    Returns:\n        Tuple of ``(filtered_peaks, log)``.\n    \"\"\"\n    log = []\n    cutoff = int(total_frames * (1.0 - end_of_video_pct))\n    mask = peaks &lt; cutoff\n    if not np.all(mask):\n        log.append(f\"End-of-video: removed {np.sum(~mask)} peak(s) past frame {cutoff}\")\n        peaks = peaks[mask]\n    return peaks, log\n</code></pre>"},{"location":"reference/filters/#end2end.filters.merge_close","title":"<code>merge_close(peaks, smoothed, min_swing_gap)</code>","text":"<p>Merge peaks closer than min_swing_gap, keeping the deeper one.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <p>1-D array of peak frame indices.</p> required <code>smoothed</code> <p>Fine-smoothed combined signal.</p> required <code>min_swing_gap</code> <p>Minimum frames between peaks.</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(merged_peaks, log)</code>.</p> Source code in <code>end2end/filters.py</code> <pre><code>def merge_close(peaks, smoothed, min_swing_gap):\n    \"\"\"Merge peaks closer than *min_swing_gap*, keeping the deeper one.\n\n    Args:\n        peaks: 1-D array of peak frame indices.\n        smoothed: Fine-smoothed combined signal.\n        min_swing_gap: Minimum frames between peaks.\n\n    Returns:\n        Tuple of ``(merged_peaks, log)``.\n    \"\"\"\n    log = []\n    merged, removed_close = [peaks[0]], []\n    for p in peaks[1:]:\n        if p - merged[-1] &lt; min_swing_gap:\n            prev = merged[-1]\n            if smoothed[p] &lt; smoothed[prev]:\n                removed_close.append(prev); merged[-1] = p\n            else:\n                removed_close.append(p)\n        else:\n            merged.append(p)\n    if removed_close:\n        log.append(f\"Too-close: removed {len(removed_close)} peak(s) within {min_swing_gap} frames\")\n    return np.array(merged), log\n</code></pre>"},{"location":"reference/filters/#end2end.filters.mad_outlier","title":"<code>mad_outlier(peaks, smoothed, mad_thresh, mad_floor, min_peaks)</code>","text":"<p>Remove peaks whose smoothed value is a MAD outlier.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <p>1-D array of peak frame indices.</p> required <code>smoothed</code> <p>Fine-smoothed combined signal.</p> required <code>mad_thresh</code> <p>MAD multiplier for the threshold.</p> required <code>mad_floor</code> <p>Minimum MAD value to prevent over-tight filtering.</p> required <code>min_peaks</code> <p>Skip filtering if fewer peaks than this.</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(filtered_peaks, log)</code>.</p> Source code in <code>end2end/filters.py</code> <pre><code>def mad_outlier(peaks, smoothed, mad_thresh, mad_floor, min_peaks):\n    \"\"\"Remove peaks whose smoothed value is a MAD outlier.\n\n    Args:\n        peaks: 1-D array of peak frame indices.\n        smoothed: Fine-smoothed combined signal.\n        mad_thresh: MAD multiplier for the threshold.\n        mad_floor: Minimum MAD value to prevent over-tight filtering.\n        min_peaks: Skip filtering if fewer peaks than this.\n\n    Returns:\n        Tuple of ``(filtered_peaks, log)``.\n    \"\"\"\n    log = []\n    if len(peaks) &lt; min_peaks:\n        return peaks, log\n    vals = smoothed[peaks]\n    med = np.median(vals)\n    mad = max(np.median(np.abs(vals - med)), mad_floor)\n    if mad &gt; 0:\n        thresh = med + mad_thresh * mad\n        om = vals &gt; thresh\n        if np.any(om):\n            log.append(f\"MAD: removed {np.sum(om)} peak(s) with x+y &gt; {thresh:.0f} (med={med:.0f}, MAD={mad:.0f})\")\n            peaks = peaks[~om]\n    return peaks, log\n</code></pre>"},{"location":"reference/filters/#end2end.filters.followthrough","title":"<code>followthrough(peaks, pkl_data, cfg)</code>","text":"<p>Reject follow-through peaks using wrist-to-shoulder horizontal offset.</p> <p>Real backswings have hands behind the body (negative offset); follow-throughs have hands in front.  Peaks with offset more than <code>xoff_mad_thresh</code> MADs above median are removed.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <p>1-D array of peak frame indices.</p> required <code>pkl_data</code> <p>Full keypoint dictionary (needed for shoulder positions).</p> required <code>cfg</code> <p><code>Config</code> instance.</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(filtered_peaks, log)</code>.</p> Source code in <code>end2end/filters.py</code> <pre><code>def followthrough(peaks, pkl_data, cfg):\n    \"\"\"Reject follow-through peaks using wrist-to-shoulder horizontal offset.\n\n    Real backswings have hands behind the body (negative offset);\n    follow-throughs have hands in front.  Peaks with offset more than\n    ``xoff_mad_thresh`` MADs above median are removed.\n\n    Args:\n        peaks: 1-D array of peak frame indices.\n        pkl_data: Full keypoint dictionary (needed for shoulder positions).\n        cfg: ``Config`` instance.\n\n    Returns:\n        Tuple of ``(filtered_peaks, log)``.\n    \"\"\"\n    log = []\n    if pkl_data is None or len(peaks) &lt; cfg.xy_outlier_min_peaks:\n        return peaks, log\n    offsets = np.empty(len(peaks))\n    for i, p in enumerate(peaks):\n        kp = np.array(pkl_data[f\"frame_{p}\"][\"keypoints\"])\n        offsets[i] = (kp[cfg.left_wrist][0] + kp[cfg.right_wrist][0]) / 2 - \\\n                     (kp[cfg.left_shoulder][0] + kp[cfg.right_shoulder][0]) / 2\n    med_off = np.median(offsets)\n    mad_off = max(np.median(np.abs(offsets - med_off)), cfg.xoff_mad_floor)\n    thresh_off = med_off + cfg.xoff_mad_thresh * mad_off\n    om = offsets &gt; thresh_off\n    if np.any(om):\n        log.append(f\"Follow-through: removed {np.sum(om)} peak(s) with x_offset &gt; {thresh_off:.0f}\")\n        peaks = peaks[~om]\n    return peaks, log\n</code></pre>"},{"location":"reference/filters/#end2end.filters.run_all","title":"<code>run_all(peaks, smoothed, total_frames, pkl_data, cfg)</code>","text":"<p>Chain all five filters in order.</p> <p>Filter order: dedup \u2192 end-of-video trim \u2192 too-close merge \u2192 MAD x+y outlier \u2192 follow-through rejection.</p> <p>Parameters:</p> Name Type Description Default <code>peaks</code> <p>1-D array of candidate peak frame indices.</p> required <code>smoothed</code> <p>Fine-smoothed combined signal.</p> required <code>total_frames</code> <p>Total video frame count.</p> required <code>pkl_data</code> <p>Full keypoint dictionary (needed for follow-through filter).</p> required <code>cfg</code> <p><code>Config</code> instance.</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(filtered_peaks, log)</code> where filtered_peaks is a 1-D</p> <p>int array and log is a list of human-readable strings describing</p> <p>what each filter removed.</p> Source code in <code>end2end/filters.py</code> <pre><code>def run_all(peaks, smoothed, total_frames, pkl_data, cfg):\n    \"\"\"Chain all five filters in order.\n\n    Filter order: dedup \u2192 end-of-video trim \u2192 too-close merge \u2192\n    MAD x+y outlier \u2192 follow-through rejection.\n\n    Args:\n        peaks: 1-D array of candidate peak frame indices.\n        smoothed: Fine-smoothed combined signal.\n        total_frames: Total video frame count.\n        pkl_data: Full keypoint dictionary (needed for follow-through\n            filter).\n        cfg: ``Config`` instance.\n\n    Returns:\n        Tuple of ``(filtered_peaks, log)`` where *filtered_peaks* is a 1-D\n        int array and *log* is a list of human-readable strings describing\n        what each filter removed.\n    \"\"\"\n    log = []\n\n    peaks, l = dedup(peaks)\n    log.extend(l)\n    if len(peaks) == 0:\n        return peaks, log\n\n    peaks, l = trim_end(peaks, total_frames, cfg.end_of_video_pct)\n    log.extend(l)\n    if len(peaks) == 0:\n        return peaks, log\n\n    peaks, l = merge_close(peaks, smoothed, cfg.min_swing_gap)\n    log.extend(l)\n\n    peaks, l = mad_outlier(peaks, smoothed, cfg.xy_outlier_mad_thresh,\n                           cfg.xy_outlier_mad_floor, cfg.xy_outlier_min_peaks)\n    log.extend(l)\n\n    peaks, l = followthrough(peaks, pkl_data, cfg)\n    log.extend(l)\n\n    return peaks, log\n</code></pre>"},{"location":"reference/io/","title":"I/O","text":"<p>Filesystem boundary \u2014 loading keypoint pickles and reading video metadata.</p>"},{"location":"reference/io/#end2end.io","title":"<code>io</code>","text":"<p>Filesystem boundary \u2014 the only module that touches disk.</p> <p>Loads mmpose keypoint pickles and reads video metadata via OpenCV.</p>"},{"location":"reference/io/#end2end.io.load_wrist_signals","title":"<code>load_wrist_signals(pkl_path, cfg)</code>","text":"<p>Load left/right wrist coordinates and confidence scores from a pickle.</p> <p>Reads the mmpose keypoint pickle produced by the pose-estimation stage and extracts the x, y, and confidence arrays for both wrists.</p> <p>Parameters:</p> Name Type Description Default <code>pkl_path</code> <p>Path to the keypoint pickle file.  Expected format is a dict mapping <code>\"frame_0\"</code> \u2026 <code>\"frame_N\"</code> to dicts with <code>\"keypoints\"</code> (17\u00d72) and <code>\"keypoint_scores\"</code> (17,) arrays.</p> required <code>cfg</code> <p><code>Config</code> instance (used for <code>left_wrist</code>, <code>right_wrist</code> keypoint indices).</p> required <p>Returns:</p> Type Description <p>Tuple of seven arrays/objects:</p> <p><code>(x_left, x_right, y_left, y_right, conf_left, conf_right, pkl_data)</code></p> <p>where each signal has shape <code>(N,)</code> and <code>pkl_data</code> is the raw dict.</p> Source code in <code>end2end/io.py</code> <pre><code>def load_wrist_signals(pkl_path, cfg):\n    \"\"\"Load left/right wrist coordinates and confidence scores from a pickle.\n\n    Reads the mmpose keypoint pickle produced by the pose-estimation stage and\n    extracts the x, y, and confidence arrays for both wrists.\n\n    Args:\n        pkl_path: Path to the keypoint pickle file.  Expected format is a dict\n            mapping ``\"frame_0\"`` \u2026 ``\"frame_N\"`` to dicts with\n            ``\"keypoints\"`` (17\u00d72) and ``\"keypoint_scores\"`` (17,) arrays.\n        cfg: ``Config`` instance (used for ``left_wrist``, ``right_wrist``\n            keypoint indices).\n\n    Returns:\n        Tuple of seven arrays/objects:\n        ``(x_left, x_right, y_left, y_right, conf_left, conf_right, pkl_data)``\n        where each signal has shape ``(N,)`` and ``pkl_data`` is the raw dict.\n    \"\"\"\n    with open(pkl_path, \"rb\") as f:\n        data = pickle.load(f)\n    n = len(data)\n    kps = np.array([data[f\"frame_{i}\"][\"keypoints\"] for i in range(n)])\n    scs = np.array([data[f\"frame_{i}\"][\"keypoint_scores\"] for i in range(n)])\n    return (kps[:, cfg.left_wrist, 0], kps[:, cfg.right_wrist, 0],\n            kps[:, cfg.left_wrist, 1], kps[:, cfg.right_wrist, 1],\n            scs[:, cfg.left_wrist], scs[:, cfg.right_wrist], data)\n</code></pre>"},{"location":"reference/io/#end2end.io.read_video_meta","title":"<code>read_video_meta(mov_path)</code>","text":"<p>Read video metadata (fps and frame count) via OpenCV.</p> <p>Parameters:</p> Name Type Description Default <code>mov_path</code> <p>Path to the source video file (<code>.MOV</code> or <code>.mp4</code>).</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(fps, total_frames)</code> where fps is a float and</p> <p>total_frames is an int.</p> Source code in <code>end2end/io.py</code> <pre><code>def read_video_meta(mov_path):\n    \"\"\"Read video metadata (fps and frame count) via OpenCV.\n\n    Args:\n        mov_path: Path to the source video file (``.MOV`` or ``.mp4``).\n\n    Returns:\n        Tuple of ``(fps, total_frames)`` where *fps* is a float and\n        *total_frames* is an int.\n    \"\"\"\n    cap = cv2.VideoCapture(mov_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.release()\n    return fps, total_frames\n</code></pre>"},{"location":"reference/peaks/","title":"Peak Detection","text":"<p>Anchor detection, backward search, scoring, and apex refinement.</p>"},{"location":"reference/peaks/#end2end.peaks","title":"<code>peaks</code>","text":"<p>Peak detection \u2014 find backswing-top frame indices from the combined signal.</p> <p>Splits the old monolithic <code>_detect_peaks</code> into composable parts: <code>find_anchors</code> \u2192 <code>search_and_refine</code> \u2192 <code>detect_peaks</code> (convenience wrapper).</p>"},{"location":"reference/peaks/#end2end.peaks.backswing_score","title":"<code>backswing_score(peak, smoothed, cfg)</code>","text":"<p>Score a candidate frame for how backswing-like it is.</p> <p>A real backswing top has a smooth approach (club slowly rising) and a sharp departure (fast downswing).  Follow-throughs are the opposite. Lower scores are more backswing-like.</p> <p>The score is::</p> <pre><code>std(diff(signal[peak - look_behind : peak]))\n- 0.5 * (signal[peak + look_ahead] - signal[peak])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>peak</code> <p>Candidate frame index.</p> required <code>smoothed</code> <p>Fine-smoothed combined signal.</p> required <code>cfg</code> <p><code>Config</code> instance (uses <code>look_behind</code>, <code>look_ahead</code>).</p> required <p>Returns:</p> Type Description <p>Float score \u2014 lower means more likely a true backswing top.</p> Source code in <code>end2end/peaks.py</code> <pre><code>def backswing_score(peak, smoothed, cfg):\n    \"\"\"Score a candidate frame for how backswing-like it is.\n\n    A real backswing top has a *smooth approach* (club slowly rising) and a\n    *sharp departure* (fast downswing).  Follow-throughs are the opposite.\n    Lower scores are more backswing-like.\n\n    The score is::\n\n        std(diff(signal[peak - look_behind : peak]))\n        - 0.5 * (signal[peak + look_ahead] - signal[peak])\n\n    Args:\n        peak: Candidate frame index.\n        smoothed: Fine-smoothed combined signal.\n        cfg: ``Config`` instance (uses ``look_behind``, ``look_ahead``).\n\n    Returns:\n        Float score \u2014 lower means more likely a true backswing top.\n    \"\"\"\n    behind = smoothed[max(0, peak - cfg.look_behind):peak]\n    approach_jitter = np.std(np.diff(behind)) if len(behind) &gt; 2 else 0.0\n    ahead = smoothed[peak:min(len(smoothed), peak + cfg.look_ahead)]\n    departure_drop = (ahead[-1] - ahead[0]) if len(ahead) &gt; 1 else 0.0\n    return approach_jitter - 0.5 * departure_drop\n</code></pre>"},{"location":"reference/peaks/#end2end.peaks.find_anchors","title":"<code>find_anchors(combined, total_video_frames, cfg)</code>","text":"<p>Fine + coarse smooth, end-of-video mask, and anchor detection.</p> <p>Parameters:</p> Name Type Description Default <code>combined</code> <p>1-D combined arc signal.</p> required <code>total_video_frames</code> <p>Total video frame count (for end-of-video masking).  May be <code>None</code> to skip masking.</p> required <code>cfg</code> <p><code>Config</code> instance.</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(anchors, smoothed, coarse)</code> where anchors is a 1-D</p> <p>int array of anchor peak indices, smoothed is the fine-smoothed</p> <p>signal, and coarse is the coarse-smoothed signal.</p> Source code in <code>end2end/peaks.py</code> <pre><code>def find_anchors(combined, total_video_frames, cfg):\n    \"\"\"Fine + coarse smooth, end-of-video mask, and anchor detection.\n\n    Args:\n        combined: 1-D combined arc signal.\n        total_video_frames: Total video frame count (for end-of-video\n            masking).  May be ``None`` to skip masking.\n        cfg: ``Config`` instance.\n\n    Returns:\n        Tuple of ``(anchors, smoothed, coarse)`` where *anchors* is a 1-D\n        int array of anchor peak indices, *smoothed* is the fine-smoothed\n        signal, and *coarse* is the coarse-smoothed signal.\n    \"\"\"\n    smoothed = savgol_filter(combined, cfg.savgol_window, cfg.savgol_poly)\n    coarse = savgol_filter(combined, cfg.coarse_window, cfg.coarse_poly)\n    neg = -smoothed\n    if total_video_frames is not None:\n        ms = int(total_video_frames * 0.95)\n        if ms &lt; len(neg):\n            neg[ms:] = np.min(neg)\n    anchors, _ = find_peaks(neg, prominence=cfg.peak_prominence, distance=cfg.peak_distance)\n    return anchors, smoothed, coarse\n</code></pre>"},{"location":"reference/peaks/#end2end.peaks.search_and_refine","title":"<code>search_and_refine(anchors, smoothed, coarse, cfg)</code>","text":"<p>Backward search, scoring, and apex refinement for each anchor.</p> <p>For each anchor, walks back up to <code>search_back</code> frames on the coarse signal to find zero-crossing candidates, scores them with <code>backswing_score</code>, and refines the winner forward on the fine signal.</p> <p>Parameters:</p> Name Type Description Default <code>anchors</code> <p>1-D array of anchor peak indices.</p> required <code>smoothed</code> <p>Fine-smoothed combined signal.</p> required <code>coarse</code> <p>Coarse-smoothed combined signal.</p> required <code>cfg</code> <p><code>Config</code> instance.</p> required <p>Returns:</p> Type Description <p>1-D int array of refined peak frame indices.</p> Source code in <code>end2end/peaks.py</code> <pre><code>def search_and_refine(anchors, smoothed, coarse, cfg):\n    \"\"\"Backward search, scoring, and apex refinement for each anchor.\n\n    For each anchor, walks back up to ``search_back`` frames on the coarse\n    signal to find zero-crossing candidates, scores them with\n    ``backswing_score``, and refines the winner forward on the fine signal.\n\n    Args:\n        anchors: 1-D array of anchor peak indices.\n        smoothed: Fine-smoothed combined signal.\n        coarse: Coarse-smoothed combined signal.\n        cfg: ``Config`` instance.\n\n    Returns:\n        1-D int array of refined peak frame indices.\n    \"\"\"\n    results = []\n    for anchor in anchors:\n        ss = max(0, anchor - cfg.search_back)\n        d = np.diff(coarse[ss:anchor + 1])\n        candidates = [ss + i + 1 for i in range(len(d) - 1) if d[i] &lt;= 0 and d[i + 1] &gt; 0]\n        if anchor not in candidates:\n            candidates.append(anchor)\n        best = candidates[np.argmin([backswing_score(c, smoothed, cfg) for c in candidates])]\n        refine = smoothed[best:min(len(smoothed), best + cfg.refine_window + 1)]\n        results.append(best + int(np.argmin(refine)))\n    return np.array(results)\n</code></pre>"},{"location":"reference/peaks/#end2end.peaks.detect_peaks","title":"<code>detect_peaks(combined, total_video_frames, cfg)</code>","text":"<p>Find backswing-top frame indices from the combined signal.</p> <p>Convenience wrapper that calls <code>find_anchors</code> then <code>search_and_refine</code>.</p> <p>Parameters:</p> Name Type Description Default <code>combined</code> <p>1-D combined arc signal.</p> required <code>total_video_frames</code> <p>Total video frame count (for end-of-video masking).  May be <code>None</code> to skip masking.</p> required <code>cfg</code> <p><code>Config</code> instance.</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(peak_frames, smoothed)</code> where peak_frames is a 1-D</p> <p>int array and smoothed is the fine-smoothed signal.</p> Source code in <code>end2end/peaks.py</code> <pre><code>def detect_peaks(combined, total_video_frames, cfg):\n    \"\"\"Find backswing-top frame indices from the combined signal.\n\n    Convenience wrapper that calls ``find_anchors`` then\n    ``search_and_refine``.\n\n    Args:\n        combined: 1-D combined arc signal.\n        total_video_frames: Total video frame count (for end-of-video\n            masking).  May be ``None`` to skip masking.\n        cfg: ``Config`` instance.\n\n    Returns:\n        Tuple of ``(peak_frames, smoothed)`` where *peak_frames* is a 1-D\n        int array and *smoothed* is the fine-smoothed signal.\n    \"\"\"\n    anchors, smoothed, coarse = find_anchors(combined, total_video_frames, cfg)\n    if len(anchors) == 0:\n        return anchors, smoothed\n    peak_frames = search_and_refine(anchors, smoothed, coarse, cfg)\n    return peak_frames, smoothed\n</code></pre>"},{"location":"reference/pipeline/","title":"Pipeline","text":"<p>Thin orchestrators connecting I/O, signal, peaks, filters, and contact.</p>"},{"location":"reference/pipeline/#end2end.pipeline","title":"<code>pipeline</code>","text":"<p>Thin orchestrators \u2014 glue I/O, signal, peaks, filters, and contact.</p> Public API <p>detect_backswings(pkl_path, mov_path, config=None) -&gt; DetectionResult detect_contacts(backswing_result, config=None) -&gt; ContactResult</p>"},{"location":"reference/pipeline/#end2end.pipeline._build_combined_signal","title":"<code>_build_combined_signal(pkl_path, cfg)</code>","text":"<p>Load wrist signals and build the combined arc signal.</p> <p>Parameters:</p> Name Type Description Default <code>pkl_path</code> <p>Path to the keypoint pickle file.</p> required <code>cfg</code> <p><code>Config</code> instance.</p> required <p>Returns:</p> Type Description <p>Tuple of <code>(combined, pkl_data)</code> where combined is a 1-D float</p> <p>array and pkl_data is the raw keypoint dict.</p> Source code in <code>end2end/pipeline.py</code> <pre><code>def _build_combined_signal(pkl_path, cfg):\n    \"\"\"Load wrist signals and build the combined arc signal.\n\n    Args:\n        pkl_path: Path to the keypoint pickle file.\n        cfg: ``Config`` instance.\n\n    Returns:\n        Tuple of ``(combined, pkl_data)`` where *combined* is a 1-D float\n        array and *pkl_data* is the raw keypoint dict.\n    \"\"\"\n    x_l, x_r, y_l, y_r, c_l, c_r, data = _io.load_wrist_signals(pkl_path, cfg)\n    combined = _signal.build_combined(x_l, x_r, y_l, y_r, c_l, c_r, cfg.conf_threshold)\n    return combined, data\n</code></pre>"},{"location":"reference/pipeline/#end2end.pipeline.detect_backswings","title":"<code>detect_backswings(pkl_path, mov_path, config=None)</code>","text":"<p>Detect top-of-backswing frames in a golf video.</p> <p>This is the main public entry point for backswing detection.  It runs the full pipeline: load keypoints \u2192 build combined signal \u2192 detect peaks \u2192 filter \u2192 return a <code>DetectionResult</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pkl_path</code> <p>Path to the mmpose keypoint pickle for this video.</p> required <code>mov_path</code> <p>Path to the source video file (<code>.MOV</code> or <code>.mp4</code>).</p> required <code>config</code> <p>Optional <code>Config</code> override.  Uses defaults if <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <p>A <code>DetectionResult</code> containing the detected peak frames, smoothed</p> <p>signals, filter log, and metadata.</p> Example <p>from end2end import detect_backswings result = detect_backswings(\"video/keypoints/IMG_1171.pkl\", ...                            \"video/IMG_1171.mp4\") print(f\"{result.n_swings} swings detected\") 4 swings detected result.peak_frames array([1527, 4166, 6433, 8337])</p> Source code in <code>end2end/pipeline.py</code> <pre><code>def detect_backswings(pkl_path, mov_path, config=None):\n    \"\"\"Detect top-of-backswing frames in a golf video.\n\n    This is the main public entry point for backswing detection.  It runs\n    the full pipeline: load keypoints \u2192 build combined signal \u2192 detect peaks\n    \u2192 filter \u2192 return a ``DetectionResult``.\n\n    Args:\n        pkl_path: Path to the mmpose keypoint pickle for this video.\n        mov_path: Path to the source video file (``.MOV`` or ``.mp4``).\n        config: Optional ``Config`` override.  Uses defaults if ``None``.\n\n    Returns:\n        A ``DetectionResult`` containing the detected peak frames, smoothed\n        signals, filter log, and metadata.\n\n    Example:\n        &gt;&gt;&gt; from end2end import detect_backswings\n        &gt;&gt;&gt; result = detect_backswings(\"video/keypoints/IMG_1171.pkl\",\n        ...                            \"video/IMG_1171.mp4\")\n        &gt;&gt;&gt; print(f\"{result.n_swings} swings detected\")\n        4 swings detected\n        &gt;&gt;&gt; result.peak_frames\n        array([1527, 4166, 6433, 8337])\n    \"\"\"\n    cfg = config or Config()\n    combined, pkl_data = _build_combined_signal(pkl_path, cfg)\n    fps, total_frames = _io.read_video_meta(mov_path)\n    peak_frames, smoothed = _peaks.detect_peaks(combined, total_frames, cfg)\n    peak_frames, flog = _filters.run_all(peak_frames, smoothed, total_frames, pkl_data, cfg)\n    return DetectionResult(\n        name=os.path.splitext(os.path.basename(mov_path))[0],\n        peak_frames=peak_frames, smoothed=smoothed, combined=combined,\n        fps=fps, total_frames=total_frames, filter_log=flog,\n        pkl_data=pkl_data, pkl_path=pkl_path, mov_path=mov_path)\n</code></pre>"},{"location":"reference/pipeline/#end2end.pipeline.detect_contacts","title":"<code>detect_contacts(backswing_result, config=None)</code>","text":"<p>Detect ball-contact frames given an existing backswing result.</p> <p>For each detected backswing, searches forward on a tightly smoothed signal for the maximum \u2014 the moment of impact where the hands are fully extended.</p> <p>Parameters:</p> Name Type Description Default <code>backswing_result</code> <p>A <code>DetectionResult</code> from <code>detect_backswings</code>.</p> required <code>config</code> <p>Optional <code>Config</code> override.  Uses defaults if <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <p>A <code>ContactResult</code> containing the contact frames, the upstream</p> <p>backswing result, and the contact-smoothed signal.</p> Example <p>from end2end import detect_backswings, detect_contacts bs = detect_backswings(\"video/keypoints/IMG_1171.pkl\", ...                        \"video/IMG_1171.mp4\") ct = detect_contacts(bs) print(f\"{ct.n_contacts} contact points\") 4 contact points</p> Source code in <code>end2end/pipeline.py</code> <pre><code>def detect_contacts(backswing_result, config=None):\n    \"\"\"Detect ball-contact frames given an existing backswing result.\n\n    For each detected backswing, searches forward on a tightly smoothed\n    signal for the maximum \u2014 the moment of impact where the hands are fully\n    extended.\n\n    Args:\n        backswing_result: A ``DetectionResult`` from ``detect_backswings``.\n        config: Optional ``Config`` override.  Uses defaults if ``None``.\n\n    Returns:\n        A ``ContactResult`` containing the contact frames, the upstream\n        backswing result, and the contact-smoothed signal.\n\n    Example:\n        &gt;&gt;&gt; from end2end import detect_backswings, detect_contacts\n        &gt;&gt;&gt; bs = detect_backswings(\"video/keypoints/IMG_1171.pkl\",\n        ...                        \"video/IMG_1171.mp4\")\n        &gt;&gt;&gt; ct = detect_contacts(bs)\n        &gt;&gt;&gt; print(f\"{ct.n_contacts} contact points\")\n        4 contact points\n    \"\"\"\n    cfg = config or Config()\n    br = backswing_result\n    cf, sm = _contact.detect_contact_points(br.peak_frames, br.combined, len(br.pkl_data), cfg)\n    cf, clog = _filters.dedup(cf)\n    return ContactResult(name=br.name, contact_frames=cf, backswing_result=br, smoothed=sm, filter_log=clog)\n</code></pre>"},{"location":"reference/run_batch/","title":"Batch Processing","text":"<p>CLI tool for running detection across full datasets.</p>"},{"location":"reference/run_batch/#end2end.run_batch","title":"<code>run_batch</code>","text":"<p>Batch backswing + contact detection CLI.</p> <p>Discovers videos in a dataset directory, runs the full detection pipeline on each, generates visualizations (grids, signal plots, optional clips), flags potential problems, and optionally exports CSV summaries.</p> <p>Usage::</p> <pre><code>python -m end2end.run_batch &lt;dataset_dir&gt; [options]\npython -m end2end.run_batch ../saugusta --no-clips --no-pushover\npython -m end2end.run_batch ../oct25 --contact --csv --no-clips --no-pushover --skip IMG_1189\n</code></pre>"},{"location":"reference/run_batch/#end2end.run_batch.discover_videos","title":"<code>discover_videos(dataset_dir, skip=None)</code>","text":"<p>Scan a dataset directory for video/keypoint pairs.</p> <p>Expects the layout::</p> <pre><code>dataset_dir/\n    VIDEO_NAME.MOV  (or .mp4)\n    VIDEO_NAME/\n        keypoints/\n            VIDEO_NAME.pkl\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <p>Root directory of the dataset.</p> required <code>skip</code> <p>Optional list of video names to exclude.</p> <code>None</code> <p>Returns:</p> Type Description <p>Dict mapping video name to <code>{\"mov\": path, \"pkl\": path}</code>, sorted</p> <p>alphabetically.</p> Source code in <code>end2end/run_batch.py</code> <pre><code>def discover_videos(dataset_dir, skip=None):\n    \"\"\"Scan a dataset directory for video/keypoint pairs.\n\n    Expects the layout::\n\n        dataset_dir/\n            VIDEO_NAME.MOV  (or .mp4)\n            VIDEO_NAME/\n                keypoints/\n                    VIDEO_NAME.pkl\n\n    Args:\n        dataset_dir: Root directory of the dataset.\n        skip: Optional list of video names to exclude.\n\n    Returns:\n        Dict mapping video name to ``{\"mov\": path, \"pkl\": path}``, sorted\n        alphabetically.\n    \"\"\"\n    skip = set(skip or [])\n    videos = {}\n    for entry in sorted(os.listdir(dataset_dir)):\n        if entry in skip:\n            continue\n        pkl = os.path.join(dataset_dir, entry, \"keypoints\", entry + \".pkl\")\n        # accept .MOV or .mp4\n        mov = os.path.join(dataset_dir, entry + \".MOV\")\n        if not os.path.isfile(mov):\n            mov = os.path.join(dataset_dir, entry + \".mp4\")\n        if os.path.isfile(pkl) and os.path.isfile(mov):\n            videos[entry] = {\"mov\": mov, \"pkl\": pkl}\n    return videos\n</code></pre>"},{"location":"reference/run_batch/#end2end.run_batch.flag_problems","title":"<code>flag_problems(result, cfg)</code>","text":"<p>Check a detection result for potential issues.</p> <p>Runs heuristic checks on the detected peaks and flags anything suspicious:</p> <ul> <li>Too few or too many swings.</li> <li>Individual peaks whose x+y value is an outlier (MAD z-score).</li> <li>Low wrist confidence in the neighbourhood of a peak.</li> <li>Two successive swings closer than <code>close_gap_seconds</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>result</code> <p><code>DetectionResult</code> to check.</p> required <code>cfg</code> <p><code>Config</code> instance (threshold parameters).</p> required <p>Returns:</p> Type Description <p>List of <code>(swing_index_or_None, reason_string)</code> tuples.  An empty</p> <p>list means no problems were found.</p> Source code in <code>end2end/run_batch.py</code> <pre><code>def flag_problems(result, cfg):\n    \"\"\"Check a detection result for potential issues.\n\n    Runs heuristic checks on the detected peaks and flags anything\n    suspicious:\n\n    - Too few or too many swings.\n    - Individual peaks whose x+y value is an outlier (MAD z-score).\n    - Low wrist confidence in the neighbourhood of a peak.\n    - Two successive swings closer than ``close_gap_seconds``.\n\n    Args:\n        result: ``DetectionResult`` to check.\n        cfg: ``Config`` instance (threshold parameters).\n\n    Returns:\n        List of ``(swing_index_or_None, reason_string)`` tuples.  An empty\n        list means no problems were found.\n    \"\"\"\n    issues = []\n    pf, smoothed, pkl_data, fps = result.peak_frames, result.smoothed, result.pkl_data, result.fps\n    n = len(pf)\n    if n &lt; cfg.min_expected_swings:\n        issues.append((None, f\"Only {n} swing(s) (expected &gt;= {cfg.min_expected_swings})\"))\n    if n &gt; cfg.max_expected_swings:\n        issues.append((None, f\"{n} swings (expected &lt;= {cfg.max_expected_swings})\"))\n    if n == 0:\n        return issues\n    vals = smoothed[pf]\n    med = np.median(vals)\n    mad = np.median(np.abs(vals - med)) if len(vals) &gt; 1 else 0.0\n    for i, p in enumerate(pf):\n        if mad &gt; 0 and len(vals) &gt;= 3:\n            z = abs(vals[i] - med) / mad\n            if z &gt; cfg.flag_mad_threshold:\n                issues.append((i, f\"x+y={vals[i]:.0f} is {z:.1f} MADs from median\"))\n        s, e = max(0, p - cfg.low_conf_window), min(len(pkl_data), p + cfg.low_conf_window + 1)\n        confs = [(pkl_data[f\"frame_{f}\"][\"keypoint_scores\"][cfg.left_wrist] +\n                  pkl_data[f\"frame_{f}\"][\"keypoint_scores\"][cfg.right_wrist]) / 2.0\n                 for f in range(s, e) if f\"frame_{f}\" in pkl_data]\n        if confs and np.mean(confs) &lt; cfg.low_conf_threshold:\n            issues.append((i, f\"Low wrist conf: {np.mean(confs):.2f}\"))\n        if i &gt; 0:\n            gap_s = (p - pf[i-1]) / fps\n            if gap_s &lt; cfg.close_gap_seconds:\n                issues.append((i, f\"Only {gap_s:.1f}s since previous swing\"))\n    return issues\n</code></pre>"},{"location":"reference/run_batch/#end2end.run_batch._write_csv","title":"<code>_write_csv(path, rows, fields)</code>","text":"<p>Write a list of row dicts to a CSV file with the given field order.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Output CSV path.</p> required <code>rows</code> <p>List of dicts, each representing one row.</p> required <code>fields</code> <p>Column names in the desired output order.</p> required Source code in <code>end2end/run_batch.py</code> <pre><code>def _write_csv(path, rows, fields):\n    \"\"\"Write a list of row dicts to a CSV file with the given field order.\n\n    Args:\n        path: Output CSV path.\n        rows: List of dicts, each representing one row.\n        fields: Column names in the desired output order.\n    \"\"\"\n    with open(path, \"w\", newline=\"\") as f:\n        w = csv.DictWriter(f, fieldnames=fields)\n        w.writeheader(); w.writerows(rows)\n    print(f\"  CSV: {path} ({len(rows)} rows)\")\n</code></pre>"},{"location":"reference/run_batch/#end2end.run_batch.export_csvs","title":"<code>export_csvs(all_bs, all_ct, out_dir)</code>","text":"<p>Export detection results to three CSV files.</p> <p>Produces:</p> <ul> <li><code>backswing_detections.csv</code> \u2014 one row per detected backswing.</li> <li><code>contact_detections.csv</code> \u2014 one row per detected contact (if any).</li> <li><code>downswing_durations.csv</code> \u2014 paired backswing\u2192contact with frame gap   and duration in seconds.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>all_bs</code> <p>List of <code>DetectionResult</code> objects.</p> required <code>all_ct</code> <p>List of <code>ContactResult</code> objects (may be empty).</p> required <code>out_dir</code> <p>Directory to write the CSV files into.</p> required Source code in <code>end2end/run_batch.py</code> <pre><code>def export_csvs(all_bs, all_ct, out_dir):\n    \"\"\"Export detection results to three CSV files.\n\n    Produces:\n\n    - ``backswing_detections.csv`` \u2014 one row per detected backswing.\n    - ``contact_detections.csv`` \u2014 one row per detected contact (if any).\n    - ``downswing_durations.csv`` \u2014 paired backswing\u2192contact with frame gap\n      and duration in seconds.\n\n    Args:\n        all_bs: List of ``DetectionResult`` objects.\n        all_ct: List of ``ContactResult`` objects (may be empty).\n        out_dir: Directory to write the CSV files into.\n    \"\"\"\n    bs_rows, ct_rows, ds_rows = [], [], []\n    for br in all_bs:\n        for i, bf in enumerate(br.peak_frames):\n            bs_rows.append(dict(video=br.name, swing_num=i+1, backswing_frame=int(bf),\n                                backswing_time_s=round(bf/br.fps,2), xy_signal=round(float(br.smoothed[bf]),1), fps=round(br.fps,2)))\n    for cr in all_ct:\n        br = cr.backswing_result\n        for i, cf in enumerate(cr.contact_frames):\n            ct_rows.append(dict(video=cr.name, swing_num=i+1, contact_frame=int(cf),\n                                contact_time_s=round(cf/br.fps,2), xy_signal=round(float(cr.smoothed[cf]),1), fps=round(br.fps,2)))\n        for i in range(min(len(br.peak_frames), len(cr.contact_frames))):\n            bf, cf = br.peak_frames[i], cr.contact_frames[i]\n            gap = int(cf - bf)\n            ds_rows.append(dict(video=cr.name, swing_num=i+1, backswing_frame=int(bf), contact_frame=int(cf),\n                                downswing_frames=gap, downswing_time_s=round(gap/br.fps,3), fps=round(br.fps,2)))\n    _write_csv(os.path.join(out_dir, \"backswing_detections.csv\"), bs_rows,\n               [\"video\",\"swing_num\",\"backswing_frame\",\"backswing_time_s\",\"xy_signal\",\"fps\"])\n    if ct_rows:\n        _write_csv(os.path.join(out_dir, \"contact_detections.csv\"), ct_rows,\n                   [\"video\",\"swing_num\",\"contact_frame\",\"contact_time_s\",\"xy_signal\",\"fps\"])\n    if ds_rows:\n        _write_csv(os.path.join(out_dir, \"downswing_durations.csv\"), ds_rows,\n                   [\"video\",\"swing_num\",\"backswing_frame\",\"contact_frame\",\"downswing_frames\",\"downswing_time_s\",\"fps\"])\n</code></pre>"},{"location":"reference/run_batch/#end2end.run_batch.main","title":"<code>main()</code>","text":"<p>Run batch detection from the command line.</p> <p>Discovers all video/keypoint pairs in the dataset directory, runs backswing (and optionally contact) detection on each, generates grids and signal plots, flags problems, and optionally exports CSVs.</p> <p>Example::</p> <pre><code># Backswing only, no clips or notifications\npython -m end2end.run_batch ../saugusta --no-clips --no-pushover\n\n# Full pipeline with contact detection and CSV export\npython -m end2end.run_batch ../oct25 --contact --csv --no-clips --no-pushover\n\n# Skip a problematic video\npython -m end2end.run_batch ../oct25 --skip IMG_1189 --no-pushover\n</code></pre> Source code in <code>end2end/run_batch.py</code> <pre><code>def main():\n    \"\"\"Run batch detection from the command line.\n\n    Discovers all video/keypoint pairs in the dataset directory, runs\n    backswing (and optionally contact) detection on each, generates grids\n    and signal plots, flags problems, and optionally exports CSVs.\n\n    Example::\n\n        # Backswing only, no clips or notifications\n        python -m end2end.run_batch ../saugusta --no-clips --no-pushover\n\n        # Full pipeline with contact detection and CSV export\n        python -m end2end.run_batch ../oct25 --contact --csv --no-clips --no-pushover\n\n        # Skip a problematic video\n        python -m end2end.run_batch ../oct25 --skip IMG_1189 --no-pushover\n    \"\"\"\n    ap = argparse.ArgumentParser(description=\"Batch backswing + contact detection\")\n    ap.add_argument(\"dataset_dir\"); ap.add_argument(\"--out\", default=None)\n    ap.add_argument(\"--contact\", action=\"store_true\"); ap.add_argument(\"--csv\", action=\"store_true\")\n    ap.add_argument(\"--no-clips\", action=\"store_true\"); ap.add_argument(\"--no-pushover\", action=\"store_true\")\n    ap.add_argument(\"--skip\", action=\"append\", default=[])\n    args = ap.parse_args()\n\n    dataset_dir = os.path.abspath(args.dataset_dir)\n    dataset_name = os.path.basename(dataset_dir)\n    out_root = os.path.abspath(args.out or (dataset_name + \"_testing\"))\n    os.makedirs(out_root, exist_ok=True)\n    cfg = Config()\n\n    videos = discover_videos(dataset_dir, skip=args.skip)\n    print(f\"Found {len(videos)} videos: {', '.join(videos.keys())}\")\n\n    if not args.no_pushover:\n        try:\n            from pushover import notify\n        except ImportError:\n            print(\"Warning: pushover not found, disabling notifications\"); args.no_pushover = True\n\n    all_bs, all_ct, all_problems = [], [], {}\n    for name, paths in videos.items():\n        vid_out = os.path.join(out_root, name)\n        os.makedirs(vid_out, exist_ok=True)\n        result = detect_backswings(paths[\"pkl\"], paths[\"mov\"], config=cfg)\n        all_bs.append(result)\n        filters = [m.split(\":\")[0] for m in result.filter_log]\n        line = f\"{name}: {result.n_swings} swings\"\n\n        ct = None\n        if args.contact:\n            ct = detect_contacts(result, config=cfg)\n            all_ct.append(ct)\n            line += f\", {ct.n_contacts} contacts\"\n        if filters:\n            line += f\"  [filters: {', '.join(filters)}]\"\n\n        # Visualizations\n        visualize.make_grid(result.peak_frames, result.pkl_data, result.mov_path, result.fps,\n                            f\"{name} \u2013 Top of Backswing\", os.path.join(vid_out, f\"{name}_backswing_grid.png\"))\n        visualize.make_signal_plot(result, vid_out)\n        if ct:\n            visualize.make_grid(ct.contact_frames, result.pkl_data, result.mov_path, result.fps,\n                                f\"{name} \u2013 Contact Points\", os.path.join(vid_out, f\"{name}_contact_grid.png\"))\n            visualize.make_signal_plot(result, vid_out, contact_result=ct)\n        if not args.no_clips:\n            visualize.extract_clips(result.peak_frames, result.mov_path, result.fps, vid_out, f\"{name}_swing\")\n            if ct:\n                visualize.extract_clips(ct.contact_frames, result.mov_path, result.fps, vid_out, f\"{name}_contact\")\n\n        issues = flag_problems(result, cfg)\n        if issues:\n            all_problems[name] = issues\n            line += \" *** PROBLEMS ***\"\n            pdir = os.path.join(out_root, \"problems\")\n            os.makedirs(pdir, exist_ok=True)\n            for suffix in [\"_backswing_grid.png\", \"_signal.png\"]:\n                src = os.path.join(vid_out, name + suffix)\n                if os.path.isfile(src):\n                    shutil.copy2(src, os.path.join(pdir, name + suffix))\n        print(line)\n\n    # Summary\n    txt = f\"\\n{dataset_name}: {sum(r.n_swings for r in all_bs)} total swings across {len(all_bs)} videos\"\n    if all_ct:\n        txt += f\", {sum(c.n_contacts for c in all_ct)} contacts\"\n    if all_problems:\n        txt += f\"\\nProblematic: {', '.join(all_problems.keys())}\"\n        pdir = os.path.join(out_root, \"problems\")\n        os.makedirs(pdir, exist_ok=True)\n        with open(os.path.join(pdir, \"summary.txt\"), \"w\") as f:\n            for vname, issues in all_problems.items():\n                f.write(f\"{vname}:\\n\")\n                for si, reason in issues:\n                    f.write(f\"  {'Swing '+str(si+1) if si is not None else 'VIDEO'}: {reason}\\n\")\n                f.write(\"\\n\")\n    print(txt)\n    if not args.no_pushover:\n        try:\n            notify(txt, title=f\"{dataset_name} detection\")\n        except Exception as e:\n            print(f\"Pushover failed: {e}\")\n    if args.csv:\n        export_csvs(all_bs, all_ct, out_root)\n    print(f\"All outputs in: {out_root}\")\n</code></pre>"},{"location":"reference/signal/","title":"Signal Processing","text":"<p>Pure signal transforms \u2014 interpolation and combined arc signal.</p>"},{"location":"reference/signal/#end2end.signal","title":"<code>signal</code>","text":"<p>Pure signal transforms \u2014 numpy in, numpy out, no side effects.</p> <p>Interpolates low-confidence keypoint frames and builds the combined arc signal used by the rest of the pipeline.</p>"},{"location":"reference/signal/#end2end.signal.interp_low_conf","title":"<code>interp_low_conf(signal, conf, threshold)</code>","text":"<p>Replace low-confidence frames with linearly interpolated values.</p> <p>mmpose occasionally drops confidence on individual frames, producing spikes in the wrist trajectory.  This replaces those frames using <code>numpy.interp</code> from the nearest high-confidence neighbors.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <p>1-D array of coordinate values (e.g. wrist x).</p> required <code>conf</code> <p>1-D confidence scores, same length as signal.</p> required <code>threshold</code> <p>Frames with confidence below this are interpolated.</p> required <p>Returns:</p> Type Description <p>A copy of signal with low-confidence values replaced.  If fewer than</p> <p>2 good frames exist, returns an unmodified copy.</p> Source code in <code>end2end/signal.py</code> <pre><code>def interp_low_conf(signal, conf, threshold):\n    \"\"\"Replace low-confidence frames with linearly interpolated values.\n\n    mmpose occasionally drops confidence on individual frames, producing\n    spikes in the wrist trajectory.  This replaces those frames using\n    ``numpy.interp`` from the nearest high-confidence neighbors.\n\n    Args:\n        signal: 1-D array of coordinate values (e.g. wrist x).\n        conf: 1-D confidence scores, same length as *signal*.\n        threshold: Frames with confidence below this are interpolated.\n\n    Returns:\n        A copy of *signal* with low-confidence values replaced.  If fewer than\n        2 good frames exist, returns an unmodified copy.\n    \"\"\"\n    bad = conf &lt; threshold\n    if not np.any(bad):\n        return signal.copy()\n    good = ~bad\n    if np.sum(good) &lt; 2:\n        return signal.copy()\n    out = signal.copy()\n    out[np.where(bad)[0]] = np.interp(np.where(bad)[0], np.where(good)[0], signal[good])\n    return out\n</code></pre>"},{"location":"reference/signal/#end2end.signal.build_combined","title":"<code>build_combined(x_l, x_r, y_l, y_r, c_l, c_r, conf_threshold)</code>","text":"<p>Build the single-channel arc signal from wrist arrays.</p> <p>Interpolates low-confidence frames on all four coordinate channels, then computes <code>(x_L + x_R) / 2 + (y_L + y_R) / 2</code>.  At the top of the backswing both x and y are low (hands high and behind); at contact they are high (hands extended forward).</p> <p>Parameters:</p> Name Type Description Default <code>x_l</code> <p>1-D array of left-wrist x coordinates.</p> required <code>x_r</code> <p>1-D array of right-wrist x coordinates.</p> required <code>y_l</code> <p>1-D array of left-wrist y coordinates.</p> required <code>y_r</code> <p>1-D array of right-wrist y coordinates.</p> required <code>c_l</code> <p>1-D array of left-wrist confidence scores.</p> required <code>c_r</code> <p>1-D array of right-wrist confidence scores.</p> required <code>conf_threshold</code> <p>Confidence threshold for interpolation.</p> required <p>Returns:</p> Type Description <p>1-D float array \u2014 the combined arc signal.</p> Source code in <code>end2end/signal.py</code> <pre><code>def build_combined(x_l, x_r, y_l, y_r, c_l, c_r, conf_threshold):\n    \"\"\"Build the single-channel arc signal from wrist arrays.\n\n    Interpolates low-confidence frames on all four coordinate channels,\n    then computes ``(x_L + x_R) / 2 + (y_L + y_R) / 2``.  At the top of\n    the backswing both x and y are low (hands high and behind); at contact\n    they are high (hands extended forward).\n\n    Args:\n        x_l: 1-D array of left-wrist x coordinates.\n        x_r: 1-D array of right-wrist x coordinates.\n        y_l: 1-D array of left-wrist y coordinates.\n        y_r: 1-D array of right-wrist y coordinates.\n        c_l: 1-D array of left-wrist confidence scores.\n        c_r: 1-D array of right-wrist confidence scores.\n        conf_threshold: Confidence threshold for interpolation.\n\n    Returns:\n        1-D float array \u2014 the combined arc signal.\n    \"\"\"\n    x_l = interp_low_conf(x_l, c_l, conf_threshold)\n    x_r = interp_low_conf(x_r, c_r, conf_threshold)\n    y_l = interp_low_conf(y_l, c_l, conf_threshold)\n    y_r = interp_low_conf(y_r, c_r, conf_threshold)\n    return (x_l + x_r) / 2.0 + (y_l + y_r) / 2.0\n</code></pre>"},{"location":"reference/visualize/","title":"Visualization","text":"<p>Skeleton overlays, signal plots, and clip extraction.</p>"},{"location":"reference/visualize/#end2end.visualize","title":"<code>visualize</code>","text":"<p>Visualization helpers \u2014 skeleton grids, signal plots, and clip extraction.</p> <p>All output is file-based (PNG images, MP4 clips).  Matplotlib uses the <code>\"Agg\"</code> backend so no display is required.</p>"},{"location":"reference/visualize/#end2end.visualize.draw_skeleton","title":"<code>draw_skeleton(frame, keypoints, scores)</code>","text":"<p>Draw a COCO-17 skeleton overlay on a video frame.</p> <p>Joints with confidence &gt; 0.1 are drawn; wrist keypoints are highlighted with large magenta circles and labelled \"L\" / \"R\".</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <p>BGR image array (H\u00d7W\u00d73).</p> required <code>keypoints</code> <p>Keypoint array of shape <code>(17, 2)</code> \u2014 <code>(x, y)</code> per joint.</p> required <code>scores</code> <p>Confidence array of shape <code>(17,)</code>.</p> required <p>Returns:</p> Type Description <p>A copy of frame with the skeleton drawn on it.</p> Source code in <code>end2end/visualize.py</code> <pre><code>def draw_skeleton(frame, keypoints, scores):\n    \"\"\"Draw a COCO-17 skeleton overlay on a video frame.\n\n    Joints with confidence &gt; 0.1 are drawn; wrist keypoints are highlighted\n    with large magenta circles and labelled \"L\" / \"R\".\n\n    Args:\n        frame: BGR image array (H\u00d7W\u00d73).\n        keypoints: Keypoint array of shape ``(17, 2)`` \u2014 ``(x, y)`` per joint.\n        scores: Confidence array of shape ``(17,)``.\n\n    Returns:\n        A copy of *frame* with the skeleton drawn on it.\n    \"\"\"\n    img = frame.copy()\n    kp = np.array(keypoints, dtype=np.int32)\n    for (a, b) in COCO_SKELETON:\n        if scores[a] &gt; 0.1 and scores[b] &gt; 0.1:\n            cv2.line(img, tuple(kp[a]), tuple(kp[b]), LIMB_COLORS.get((a,b),(200,200,200)), 2, cv2.LINE_AA)\n    for i, (pt, sc) in enumerate(zip(kp, scores)):\n        if sc &gt; 0.1:\n            c, r = ((255,0,255), 12) if i in (_LW, _RW) else ((0,255,0), 4)\n            cv2.circle(img, tuple(pt), r, c, -1, cv2.LINE_AA)\n    for idx, label in [(_LW, \"L\"), (_RW, \"R\")]:\n        if scores[idx] &gt; 0.1:\n            cv2.putText(img, label, (kp[idx][0]+15, kp[idx][1]-10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,0,255), 2, cv2.LINE_AA)\n    return img\n</code></pre>"},{"location":"reference/visualize/#end2end.visualize.make_grid","title":"<code>make_grid(frames, pkl_data, mov_path, fps, title, out_path)</code>","text":"<p>Render a grid of skeleton-overlaid video frames and save as PNG.</p> <p>Arranges frames in a 4-column grid.  Each cell shows the video frame at the given index with the COCO-17 skeleton drawn on top.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <p>1-D array of frame indices to render.</p> required <code>pkl_data</code> <p>Keypoint dictionary (<code>frame_N</code> \u2192 keypoints/scores).</p> required <code>mov_path</code> <p>Path to the source video file.</p> required <code>fps</code> <p>Video frame rate (used in per-cell time labels).</p> required <code>title</code> <p>Figure suptitle.</p> required <code>out_path</code> <p>Destination PNG path.</p> required <p>Returns:</p> Type Description <p>out_path on success, or <code>None</code> if frames is empty.</p> Source code in <code>end2end/visualize.py</code> <pre><code>def make_grid(frames, pkl_data, mov_path, fps, title, out_path):\n    \"\"\"Render a grid of skeleton-overlaid video frames and save as PNG.\n\n    Arranges frames in a 4-column grid.  Each cell shows the video frame at\n    the given index with the COCO-17 skeleton drawn on top.\n\n    Args:\n        frames: 1-D array of frame indices to render.\n        pkl_data: Keypoint dictionary (``frame_N`` \u2192 keypoints/scores).\n        mov_path: Path to the source video file.\n        fps: Video frame rate (used in per-cell time labels).\n        title: Figure suptitle.\n        out_path: Destination PNG path.\n\n    Returns:\n        *out_path* on success, or ``None`` if *frames* is empty.\n    \"\"\"\n    if len(frames) == 0:\n        return None\n    cap = cv2.VideoCapture(mov_path)\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    ncols = 4\n    nrows = max(1, int(np.ceil(len(frames) / ncols)))\n    fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 8*nrows))\n    if nrows == 1:\n        axes = np.array(axes).reshape(1, -1)\n    axes = axes.flatten()\n    for i, pf in enumerate(frames):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, min(pf, total-1))\n        ret, frame = cap.read()\n        if not ret:\n            axes[i].set_title(f\"Frame {pf} (read failed)\"); axes[i].axis(\"off\"); continue\n        fd = pkl_data[f\"frame_{pf}\"]\n        axes[i].imshow(cv2.cvtColor(draw_skeleton(frame, fd[\"keypoints\"], fd[\"keypoint_scores\"]), cv2.COLOR_BGR2RGB))\n        axes[i].set_title(f\"Frame {pf}  ({pf/fps:.1f}s)\", fontsize=11, fontweight=\"bold\")\n        axes[i].axis(\"off\")\n    for j in range(len(frames), len(axes)):\n        axes[j].axis(\"off\")\n    fig.suptitle(title, fontsize=16, fontweight=\"bold\")\n    fig.tight_layout(rect=[0,0,1,0.97])\n    fig.savefig(out_path, dpi=90, bbox_inches=\"tight\")\n    plt.close(fig); cap.release()\n    return out_path\n</code></pre>"},{"location":"reference/visualize/#end2end.visualize.make_signal_plot","title":"<code>make_signal_plot(result, out_dir, contact_result=None)</code>","text":"<p>Plot the combined signal with detected landmarks and save as PNG.</p> <p>Shows the raw and smoothed combined signal with backswing tops marked as red triangles.  If contact_result is provided, contact points are added as green triangles with arrows connecting each backswing\u2013contact pair.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <p><code>DetectionResult</code> for the video.</p> required <code>out_dir</code> <p>Directory to save the output PNG.</p> required <code>contact_result</code> <p>Optional <code>ContactResult</code>.  When provided the plot uses its smoother and includes contact markers.</p> <code>None</code> <p>Returns:</p> Type Description <p>Path to the saved PNG file.</p> Source code in <code>end2end/visualize.py</code> <pre><code>def make_signal_plot(result, out_dir, contact_result=None):\n    \"\"\"Plot the combined signal with detected landmarks and save as PNG.\n\n    Shows the raw and smoothed combined signal with backswing tops marked as\n    red triangles.  If *contact_result* is provided, contact points are added\n    as green triangles with arrows connecting each backswing\u2013contact pair.\n\n    Args:\n        result: ``DetectionResult`` for the video.\n        out_dir: Directory to save the output PNG.\n        contact_result: Optional ``ContactResult``.  When provided the plot\n            uses its smoother and includes contact markers.\n\n    Returns:\n        Path to the saved PNG file.\n    \"\"\"\n    br = contact_result.backswing_result if contact_result else result\n    t = np.arange(len(br.combined)) / br.fps\n    sm = contact_result.smoothed if contact_result else result.smoothed\n    fig, ax = plt.subplots(figsize=(16, 5))\n    ax.plot(t, br.combined, color=\"lightgray\", linewidth=0.5, label=\"Raw (x+y)\")\n    ax.plot(t, sm, color=\"steelblue\", linewidth=1.2, label=\"Smoothed\")\n    ax.invert_yaxis()\n    pf = br.peak_frames\n    if len(pf) &gt; 0:\n        ax.plot(pf/br.fps, sm[pf], \"rv\", markersize=10, label=\"Backswing top\")\n        for p in pf:\n            ax.axvline(p/br.fps, color=\"red\", alpha=0.3, linewidth=0.8)\n            ax.annotate(str(p), (p/br.fps, sm[p]), textcoords=\"offset points\", xytext=(5,-15), fontsize=8, color=\"red\")\n    if contact_result is not None and len(contact_result.contact_frames) &gt; 0:\n        cf = contact_result.contact_frames\n        ax.plot(cf/br.fps, sm[cf], \"g^\", markersize=10, label=\"Contact\")\n        for c in cf:\n            ax.axvline(c/br.fps, color=\"lime\", alpha=0.2, linewidth=0.8)\n            ax.annotate(str(c), (c/br.fps, sm[c]), textcoords=\"offset points\", xytext=(5,10), fontsize=8, color=\"lime\")\n        bs = contact_result.backswing_result.peak_frames\n        for i in range(min(len(bs), len(cf))):\n            ax.annotate(\"\", xy=(cf[i]/br.fps, sm[cf[i]]), xytext=(bs[i]/br.fps, sm[bs[i]]),\n                        arrowprops=dict(arrowstyle=\"-&gt;\", color=\"yellow\", alpha=0.4, lw=1.5))\n    ax.set_xlabel(\"Time (s)\"); ax.set_ylabel(\"Arc position x+y (inverted)\")\n    ax.set_title(f\"{br.name} \u2013 Backswing\" + (\" &amp; Contact\" if contact_result else \"\"))\n    ax.legend(loc=\"lower right\"); fig.tight_layout()\n    suffix = \"_contact_signal.png\" if contact_result else \"_signal.png\"\n    out_path = os.path.join(out_dir, br.name + suffix)\n    fig.savefig(out_path, dpi=120); plt.close(fig)\n    return out_path\n</code></pre>"},{"location":"reference/visualize/#end2end.visualize.extract_clips","title":"<code>extract_clips(frames, mov_path, fps, out_dir, prefix)</code>","text":"<p>Extract short MP4 clips centred on each detected frame.</p> <p>Each clip spans \u00b10.5 s around the target frame and is written with the <code>mp4v</code> codec at the original frame rate.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <p>1-D array of centre-frame indices.</p> required <code>mov_path</code> <p>Path to the source video file.</p> required <code>fps</code> <p>Video frame rate.</p> required <code>out_dir</code> <p>Directory to write clips into.</p> required <code>prefix</code> <p>Filename prefix \u2014 clips are named <code>{prefix}_01.mp4</code>, <code>{prefix}_02.mp4</code>, etc.</p> required <p>Returns:</p> Type Description <p>List of paths to the written MP4 files, or an empty list if</p> <p>frames is empty.</p> Source code in <code>end2end/visualize.py</code> <pre><code>def extract_clips(frames, mov_path, fps, out_dir, prefix):\n    \"\"\"Extract short MP4 clips centred on each detected frame.\n\n    Each clip spans \u00b10.5 s around the target frame and is written with the\n    ``mp4v`` codec at the original frame rate.\n\n    Args:\n        frames: 1-D array of centre-frame indices.\n        mov_path: Path to the source video file.\n        fps: Video frame rate.\n        out_dir: Directory to write clips into.\n        prefix: Filename prefix \u2014 clips are named\n            ``{prefix}_01.mp4``, ``{prefix}_02.mp4``, etc.\n\n    Returns:\n        List of paths to the written MP4 files, or an empty list if\n        *frames* is empty.\n    \"\"\"\n    if len(frames) == 0:\n        return []\n    cap = cv2.VideoCapture(mov_path)\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    half_sec = int(round(fps * 0.5))\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    paths = []\n    for i, pf in enumerate(frames):\n        clamped = min(pf, total-1)\n        start, end = max(0, clamped-half_sec), min(total, clamped+half_sec+1)\n        out_path = os.path.join(out_dir, f\"{prefix}_{i+1:02d}.mp4\")\n        writer = cv2.VideoWriter(out_path, fourcc, fps, (w, h))\n        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n        for _ in range(end - start):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            writer.write(frame)\n        writer.release()\n        paths.append(out_path)\n    cap.release()\n    return paths\n</code></pre>"}]}